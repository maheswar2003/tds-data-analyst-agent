"""
ULTIMATE Optimized Core Agent Logic for TDS Data Analyst Agent.

This module contains the most advanced agent functionality with:
- Comprehensive system prompt with multiple examples
- Advanced pattern recognition for all data types
- Enhanced security and validation
- Intelligent retry logic with exponential backoff
- Perfect JSON formatting
- Maximum performance optimization
"""

import os
import sys
import subprocess
import tempfile
import json
import ast
import time
import re
from typing import Dict, Any, Optional, List
import logging
from dataclasses import dataclass

# Import Google Generative AI with retry support
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    genai = None

logger = logging.getLogger(__name__)


@dataclass
class AgentConfig:
    """Configuration for the Ultimate Data Analyst Agent."""
    gemini_model: str = "gemini-1.5-pro"
    max_tokens: int = 4000  # Increased for comprehensive analysis
    temperature: float = 0.2  # Lower for more consistent output
    execution_timeout: int = 180  # Increased timeout
    max_retries: int = 3
    retry_delay: float = 1.0
    enable_code_validation: bool = True
    max_memory_mb: int = 512
    enable_smart_parsing: bool = True


def validate_generated_code(code: str) -> bool:
    """
    Advanced validation of generated code for safety and syntax.
    
    Args:
        code: Python code to validate
        
    Returns:
        True if code is valid and safe
    """
    # Check syntax
    try:
        ast.parse(code)
    except SyntaxError:
        logger.error("Code has syntax errors")
        return False
    
    # Check for dangerous imports (allow pandas file reading though)
    dangerous_imports = {'os', 'subprocess', 'sys', 'eval', 'exec', '__import__'}
    tree = ast.parse(code)
    
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                if alias.name in dangerous_imports:
                    logger.warning(f"Dangerous import detected: {alias.name}")
                    return False
        elif isinstance(node, ast.ImportFrom):
            if node.module in dangerous_imports:
                logger.warning(f"Dangerous import detected: {node.module}")
                return False
    
    # Ensure code has print statement for JSON output
    if 'print(json.dumps(' not in code:
        logger.warning("Code missing JSON output print statement")
        return False
    
    return True


def extract_json_from_output(output: str) -> str:
    """
    Smart extraction of JSON from potentially mixed output.
    
    Args:
        output: Raw output that may contain JSON
        
    Returns:
        Extracted JSON string
    """
    # Try to find JSON in the output
    lines = output.strip().split('\n')
    
    # Look for the last line that looks like JSON
    for line in reversed(lines):
        line = line.strip()
        if line.startswith('{') and line.endswith('}'):
            try:
                json.loads(line)
                return line
            except:
                continue
    
    # Try to extract JSON from the entire output
    json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
    matches = re.findall(json_pattern, output, re.DOTALL)
    
    for match in reversed(matches):
        try:
            json.loads(match)
            return match
        except:
            continue
    
    return output


def generate_analysis_script(
    task_description: str, 
    config: Optional[AgentConfig] = None
) -> str:
    """
    Generate the ULTIMATE Python script using Google Gemini API.
    
    Args:
        task_description: Natural language description of the analysis task
        config: Optional configuration object
        
    Returns:
        Generated Python script as a string
        
    Raises:
        Exception: If Google Gemini API is not available or all retries fail
    """
    if not GEMINI_AVAILABLE:
        raise Exception("Google Generative AI library not installed. Please install with: pip install google-generativeai")
    
    api_key = os.getenv('GOOGLE_API_KEY')
    if not api_key:
        raise Exception("GOOGLE_API_KEY environment variable not set")
    
    if config is None:
        config = AgentConfig()
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(config.gemini_model)
    
    # ULTIMATE COMPREHENSIVE SYSTEM PROMPT
    system_prompt = """
You are the world's best data analyst and Python programmer. Generate PERFECT Python scripts for data analysis.

ABSOLUTE CRITICAL REQUIREMENTS:
1. Import ALL necessary libraries at the top
2. Handle all data operations comprehensively
3. Create professional visualizations with proper formatting
4. Output MUST be valid JSON on the LAST line using print(json.dumps())
5. Include complete error handling
6. Make the script 100% self-contained

FILE HANDLING RULES:
- If user mentions an attached file, assume it exists as 'data.csv' or 'data.xlsx'
- Always use try/except when reading files
- Handle missing columns gracefully

VISUALIZATION REQUIREMENTS:
```python
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import io
import base64

# Create plot
plt.figure(figsize=(10, 6))
# ... plotting code ...
plt.tight_layout()

# Convert to base64
buf = io.BytesIO()
plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
plt.close()
buf.seek(0)
plot_base64 = base64.b64encode(buf.read()).decode('utf-8')
plot_uri = f"data:image/png;base64,{plot_base64}"
```

REQUIRED JSON OUTPUT FORMAT:
```python
result = {
    "summary": "Comprehensive analysis summary",
    "data": {
        # All computed metrics and results
    },
    "visualizations": [plot_uri],  # List of base64 encoded plots
    "insights": [
        "Key insight 1",
        "Key insight 2",
        "Key insight 3"
    ],
    "metadata": {
        "rows": number_of_rows,
        "columns": number_of_columns,
        "analysis_type": "type_of_analysis"
    },
    "status": "success",
    "error": null
}
print(json.dumps(result))
```

EXAMPLE 1 - SALES DATA ANALYSIS:
```python
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import io
import base64
import json
from datetime import datetime

try:
    # Read data
    df = pd.read_csv('data.csv')
    
    # Clean and prepare data
    df = df.dropna()
    
    # Analysis
    total_sales = df['sales'].sum()
    avg_sales = df['sales'].mean()
    top_products = df.nlargest(5, 'sales')[['product', 'sales']]
    monthly_sales = df.groupby('month')['sales'].sum()
    
    # Visualization
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # Sales by product
    axes[0, 0].bar(top_products['product'], top_products['sales'])
    axes[0, 0].set_title('Top 5 Products by Sales')
    axes[0, 0].set_xlabel('Product')
    axes[0, 0].set_ylabel('Sales ($)')
    
    # Monthly trend
    axes[0, 1].plot(monthly_sales.index, monthly_sales.values, marker='o')
    axes[0, 1].set_title('Monthly Sales Trend')
    axes[0, 1].set_xlabel('Month')
    axes[0, 1].set_ylabel('Sales ($)')
    
    # Distribution
    axes[1, 0].hist(df['sales'], bins=20, edgecolor='black')
    axes[1, 0].set_title('Sales Distribution')
    axes[1, 0].set_xlabel('Sales Amount')
    axes[1, 0].set_ylabel('Frequency')
    
    # Pie chart
    category_sales = df.groupby('category')['sales'].sum()
    axes[1, 1].pie(category_sales.values, labels=category_sales.index, autopct='%1.1f%%')
    axes[1, 1].set_title('Sales by Category')
    
    plt.tight_layout()
    
    # Convert to base64
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
    plt.close()
    buf.seek(0)
    plot_base64 = base64.b64encode(buf.read()).decode('utf-8')
    
    # Results
    result = {
        "summary": f"Sales Analysis Complete: Total ${total_sales:,.2f}, Average ${avg_sales:,.2f}",
        "data": {
            "total_sales": float(total_sales),
            "average_sales": float(avg_sales),
            "top_products": top_products.to_dict('records'),
            "monthly_sales": monthly_sales.to_dict(),
            "category_breakdown": category_sales.to_dict()
        },
        "visualizations": [f"data:image/png;base64,{plot_base64}"],
        "insights": [
            f"Total sales revenue: ${total_sales:,.2f}",
            f"Average sale amount: ${avg_sales:,.2f}",
            f"Best performing product: {top_products.iloc[0]['product']}",
            f"Number of unique products: {df['product'].nunique()}"
        ],
        "metadata": {
            "rows": len(df),
            "columns": len(df.columns),
            "analysis_type": "sales_analysis"
        },
        "status": "success",
        "error": null
    }
    
    print(json.dumps(result))
    
except Exception as e:
    error_result = {
        "summary": f"Analysis failed: {str(e)}",
        "data": {},
        "visualizations": [],
        "insights": [],
        "metadata": {},
        "status": "error",
        "error": str(e)
    }
    print(json.dumps(error_result))
```

EXAMPLE 2 - WEATHER DATA ANALYSIS:
```python
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import io
import base64
import json

try:
    # Create or read weather data
    df = pd.read_csv('data.csv')
    
    # Temperature analysis
    avg_temp = df['temperature'].mean()
    max_temp = df['temperature'].max()
    min_temp = df['temperature'].min()
    temp_range = max_temp - min_temp
    
    # Find hottest and coldest days
    hottest_day = df.loc[df['temperature'].idxmax()]
    coldest_day = df.loc[df['temperature'].idxmin()]
    
    # Calculate daily statistics
    daily_stats = df.groupby('date').agg({
        'temperature': ['mean', 'max', 'min'],
        'humidity': 'mean',
        'precipitation': 'sum'
    })
    
    # Create comprehensive visualization
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Temperature trend
    axes[0, 0].plot(df['date'], df['temperature'], color='red', linewidth=2)
    axes[0, 0].axhline(y=avg_temp, color='blue', linestyle='--', label=f'Avg: {avg_temp:.1f}Â°')
    axes[0, 0].set_title('Temperature Trend')
    axes[0, 0].set_xlabel('Date')
    axes[0, 0].set_ylabel('Temperature (Â°C)')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Temperature distribution
    axes[0, 1].hist(df['temperature'], bins=15, color='skyblue', edgecolor='black')
    axes[0, 1].axvline(x=avg_temp, color='red', linestyle='--')
    axes[0, 1].set_title('Temperature Distribution')
    axes[0, 1].set_xlabel('Temperature (Â°C)')
    axes[0, 1].set_ylabel('Frequency')
    
    # Humidity vs Temperature
    axes[1, 0].scatter(df['temperature'], df['humidity'], alpha=0.6)
    axes[1, 0].set_title('Temperature vs Humidity')
    axes[1, 0].set_xlabel('Temperature (Â°C)')
    axes[1, 0].set_ylabel('Humidity (%)')
    
    # Precipitation
    axes[1, 1].bar(df['date'], df['precipitation'], color='blue', alpha=0.7)
    axes[1, 1].set_title('Daily Precipitation')
    axes[1, 1].set_xlabel('Date')
    axes[1, 1].set_ylabel('Precipitation (mm)')
    
    plt.tight_layout()
    
    # Convert to base64
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
    plt.close()
    buf.seek(0)
    plot_base64 = base64.b64encode(buf.read()).decode('utf-8')
    
    # Generate insights
    insights = [
        f"Average temperature: {avg_temp:.1f}Â°C",
        f"Temperature range: {temp_range:.1f}Â°C (from {min_temp:.1f}Â°C to {max_temp:.1f}Â°C)",
        f"Hottest day: {hottest_day['date']} at {hottest_day['temperature']:.1f}Â°C",
        f"Coldest day: {coldest_day['date']} at {coldest_day['temperature']:.1f}Â°C",
        f"Average humidity: {df['humidity'].mean():.1f}%",
        f"Total precipitation: {df['precipitation'].sum():.1f}mm"
    ]
    
    result = {
        "summary": f"Weather Analysis: Avg temp {avg_temp:.1f}Â°C, Range {temp_range:.1f}Â°C",
        "data": {
            "average_temperature": float(avg_temp),
            "max_temperature": float(max_temp),
            "min_temperature": float(min_temp),
            "temperature_range": float(temp_range),
            "average_humidity": float(df['humidity'].mean()),
            "total_precipitation": float(df['precipitation'].sum()),
            "hottest_day": {
                "date": str(hottest_day['date']),
                "temperature": float(hottest_day['temperature'])
            },
            "coldest_day": {
                "date": str(coldest_day['date']),
                "temperature": float(coldest_day['temperature'])
            }
        },
        "visualizations": [f"data:image/png;base64,{plot_base64}"],
        "insights": insights,
        "metadata": {
            "rows": len(df),
            "columns": len(df.columns),
            "analysis_type": "weather_analysis",
            "date_range": f"{df['date'].min()} to {df['date'].max()}"
        },
        "status": "success",
        "error": null
    }
    
    print(json.dumps(result))
    
except Exception as e:
    error_result = {
        "summary": f"Weather analysis failed: {str(e)}",
        "data": {},
        "visualizations": [],
        "insights": [],
        "metadata": {},
        "status": "error",
        "error": str(e)
    }
    print(json.dumps(error_result))
```

EXAMPLE 3 - NETWORK DATA ANALYSIS:
```python
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import networkx as nx
import io
import base64
import json

try:
    # Read network data
    df = pd.read_csv('data.csv')
    
    # Network metrics
    total_traffic = df['bytes'].sum()
    avg_latency = df['latency'].mean()
    packet_loss = (df['packets_lost'].sum() / df['packets_sent'].sum()) * 100
    
    # Peak traffic analysis
    peak_hour = df.groupby('hour')['bytes'].sum().idxmax()
    peak_traffic = df.groupby('hour')['bytes'].sum().max()
    
    # Node analysis
    top_sources = df.groupby('source')['bytes'].sum().nlargest(5)
    top_destinations = df.groupby('destination')['bytes'].sum().nlargest(5)
    
    # Create visualizations
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Traffic over time
    hourly_traffic = df.groupby('hour')['bytes'].sum()
    axes[0, 0].plot(hourly_traffic.index, hourly_traffic.values, marker='o', linewidth=2)
    axes[0, 0].fill_between(hourly_traffic.index, hourly_traffic.values, alpha=0.3)
    axes[0, 0].set_title('Network Traffic Over Time')
    axes[0, 0].set_xlabel('Hour')
    axes[0, 0].set_ylabel('Traffic (bytes)')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Latency distribution
    axes[0, 1].hist(df['latency'], bins=20, color='orange', edgecolor='black')
    axes[0, 1].axvline(x=avg_latency, color='red', linestyle='--', label=f'Avg: {avg_latency:.2f}ms')
    axes[0, 1].set_title('Latency Distribution')
    axes[0, 1].set_xlabel('Latency (ms)')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].legend()
    
    # Top sources
    axes[1, 0].barh(top_sources.index, top_sources.values, color='green')
    axes[1, 0].set_title('Top 5 Traffic Sources')
    axes[1, 0].set_xlabel('Traffic (bytes)')
    axes[1, 0].set_ylabel('Source')
    
    # Protocol distribution
    protocol_dist = df.groupby('protocol')['bytes'].sum()
    axes[1, 1].pie(protocol_dist.values, labels=protocol_dist.index, autopct='%1.1f%%')
    axes[1, 1].set_title('Traffic by Protocol')
    
    plt.tight_layout()
    
    # Convert to base64
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
    plt.close()
    buf.seek(0)
    plot_base64 = base64.b64encode(buf.read()).decode('utf-8')
    
    # Calculate additional metrics
    bandwidth_utilization = (total_traffic / (df['bandwidth'].mean() * len(df))) * 100
    
    result = {
        "summary": f"Network Analysis: {total_traffic/1e9:.2f}GB traffic, {avg_latency:.2f}ms avg latency",
        "data": {
            "total_traffic_bytes": int(total_traffic),
            "total_traffic_gb": float(total_traffic / 1e9),
            "average_latency_ms": float(avg_latency),
            "packet_loss_percentage": float(packet_loss),
            "peak_traffic_hour": int(peak_hour),
            "peak_traffic_bytes": int(peak_traffic),
            "bandwidth_utilization": float(bandwidth_utilization),
            "top_sources": top_sources.to_dict(),
            "top_destinations": top_destinations.to_dict(),
            "protocol_distribution": protocol_dist.to_dict()
        },
        "visualizations": [f"data:image/png;base64,{plot_base64}"],
        "insights": [
            f"Total network traffic: {total_traffic/1e9:.2f} GB",
            f"Average latency: {avg_latency:.2f} ms",
            f"Packet loss rate: {packet_loss:.2f}%",
            f"Peak traffic hour: {peak_hour}:00 with {peak_traffic/1e6:.2f} MB",
            f"Bandwidth utilization: {bandwidth_utilization:.1f}%",
            f"Top traffic source: {top_sources.index[0]}"
        ],
        "metadata": {
            "rows": len(df),
            "columns": len(df.columns),
            "analysis_type": "network_analysis",
            "unique_sources": df['source'].nunique(),
            "unique_destinations": df['destination'].nunique()
        },
        "status": "success",
        "error": null
    }
    
    print(json.dumps(result))
    
except Exception as e:
    error_result = {
        "summary": f"Network analysis failed: {str(e)}",
        "data": {},
        "visualizations": [],
        "insights": [],
        "metadata": {},
        "status": "error",
        "error": str(e)
    }
    print(json.dumps(error_result))
```

CRITICAL DATA ANALYSIS PATTERNS:

1. SALES DATA: Always calculate totals, averages, top products, trends, category breakdowns
2. WEATHER DATA: Always show temperature trends, statistics, hottest/coldest days, precipitation
3. NETWORK DATA: Always analyze traffic patterns, latency, packet loss, top sources/destinations
4. FINANCIAL DATA: Calculate returns, volatility, moving averages, portfolio metrics
5. CUSTOMER DATA: Segment analysis, churn rates, lifetime value, satisfaction scores
6. INVENTORY DATA: Stock levels, turnover rates, reorder points, ABC analysis
7. MARKETING DATA: Conversion rates, ROI, channel performance, A/B test results
8. TIME SERIES: Trends, seasonality, forecasts, anomaly detection

MANDATORY SUCCESS CRITERIA:
1. ALWAYS generate valid, executable Python code
2. ALWAYS include comprehensive error handling
3. ALWAYS create professional visualizations
4. ALWAYS output valid JSON on the last line
5. ALWAYS include meaningful insights
6. ALWAYS handle edge cases gracefully
7. NEVER leave analysis incomplete

Generate the PERFECT analysis script that will impress with its thoroughness and accuracy.
"""
    
    # Retry logic with exponential backoff
    last_error = None
    for attempt in range(config.max_retries):
        try:
            # Enhanced prompt construction
            prompt = f"""{system_prompt}

TASK: {task_description}

Remember:
- If data file is mentioned, use 'data.csv' or appropriate extension
- Include ALL necessary imports
- Create comprehensive visualizations
- Output MUST be valid JSON using print(json.dumps())
- Handle all errors gracefully

Generate the complete Python script now:"""
            
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=config.max_tokens,
                    temperature=config.temperature,
                )
            )
            
            generated_script = response.text
            
            # Clean up the response
            if "```python" in generated_script:
                start = generated_script.find("```python") + 9
                end = generated_script.rfind("```")
                if end > start:
                    generated_script = generated_script[start:end].strip()
            elif "```" in generated_script:
                start = generated_script.find("```") + 3
                end = generated_script.rfind("```")
                if end > start:
                    generated_script = generated_script[start:end].strip()
            
            # Validate if enabled
            if config.enable_code_validation:
                if not validate_generated_code(generated_script):
                    # Try to fix common issues
                    if 'print(json.dumps(' not in generated_script:
                        # Add JSON output if missing
                        generated_script += '\n\n# Ensure JSON output\nif "result" in locals():\n    print(json.dumps(result))'
                    
                    # Re-validate
                    if not validate_generated_code(generated_script):
                        raise Exception("Generated code failed validation after fixes")
            
            logger.info(f"Successfully generated script on attempt {attempt + 1}")
            return generated_script
            
        except Exception as e:
            last_error = e
            logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")
            if attempt < config.max_retries - 1:
                time.sleep(config.retry_delay * (2 ** attempt))  # Exponential backoff
            continue
    
    raise Exception(f"Failed after {config.max_retries} attempts. Last error: {str(last_error)}")


def execute_script(
    script_code: str, 
    config: Optional[AgentConfig] = None
) -> str:
    """
    Execute Python script with enhanced safety and output parsing.
    
    Args:
        script_code: The Python script code to execute
        config: Optional configuration object
        
    Returns:
        The stdout output from the script execution
        
    Raises:
        Exception: If script execution fails or times out
    """
    if config is None:
        config = AgentConfig()
    
    try:
        # Create a temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp_file:
            temp_file.write(script_code)
            temp_file_path = temp_file.name
        
        try:
            # Prepare subprocess with resource limits
            env = os.environ.copy()
            
            # Execute the script
            result = subprocess.run(
                [sys.executable, temp_file_path],
                capture_output=True,
                text=True,
                timeout=config.execution_timeout,
                cwd=os.getcwd(),
                env=env
            )
            
            if result.returncode == 0:
                output = result.stdout.strip()
                
                # Smart JSON extraction if enabled
                if config.enable_smart_parsing:
                    output = extract_json_from_output(output)
                
                return output
            else:
                # Try to extract JSON from error output
                combined_output = result.stdout + result.stderr
                if config.enable_smart_parsing and '{' in combined_output:
                    try:
                        json_output = extract_json_from_output(combined_output)
                        return json_output
                    except:
                        pass
                
                error_msg = f"Script execution failed with return code {result.returncode}\n"
                error_msg += f"STDERR: {result.stderr}\n"
                error_msg += f"STDOUT: {result.stdout}"
                raise Exception(error_msg)
                
        finally:
            # Clean up
            try:
                os.unlink(temp_file_path)
            except OSError:
                pass
                
    except subprocess.TimeoutExpired:
        raise Exception(f"Script execution timed out after {config.execution_timeout} seconds")
    except Exception as e:
        if "Script execution failed" in str(e) or "timed out" in str(e):
            raise
        else:
            raise Exception(f"Error executing script: {str(e)}")


class DataAnalystAgent:
    """
    ULTIMATE optimized data analyst agent with maximum capabilities.
    """
    
    def __init__(self, config: Optional[AgentConfig] = None):
        """Initialize the agent with optimal configuration."""
        self.config = config or AgentConfig()
        logger.info(f"Initialized ULTIMATE agent with config: {self.config}")
    
    async def process_question(
        self, 
        question: str, 
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Process any data analysis question with maximum accuracy.
        
        Args:
            question: The natural language question to analyze
            context: Optional context dictionary
            
        Returns:
            Dict containing code, results, and comprehensive analysis
        """
        logger.info(f"Processing question: {question[:100]}...")
        
        try:
            # Generate optimal code
            generated_code = generate_analysis_script(question, self.config)
            logger.info(f"Generated {len(generated_code)} characters of optimized code")
            
            # Execute with enhanced safety
            json_output = execute_script(generated_code, self.config)
            logger.info("Code execution successful")
            
            # Parse output with validation
            try:
                parsed_output = json.loads(json_output)
                
                # Ensure output has all required fields
                required_fields = ['summary', 'data', 'visualizations', 'insights', 'metadata', 'status']
                for field in required_fields:
                    if field not in parsed_output:
                        parsed_output[field] = [] if field in ['visualizations', 'insights'] else {}
                
                return {
                    "code": generated_code,
                    "output": parsed_output,
                    "explanation": "Analysis completed successfully with comprehensive results.",
                    "execution_success": True,
                    "config": {
                        "model": self.config.gemini_model,
                        "timeout": self.config.execution_timeout,
                        "optimization": "maximum"
                    }
                }
            except json.JSONDecodeError as e:
                logger.warning(f"JSON parsing failed: {e}")
                
                # Try to create a valid response
                return {
                    "code": generated_code,
                    "output": {
                        "summary": "Analysis completed",
                        "data": {"raw_output": json_output[:1000]},
                        "visualizations": [],
                        "insights": ["Analysis produced non-JSON output"],
                        "metadata": {"format": "text"},
                        "status": "partial",
                        "error": "Output not in expected JSON format"
                    },
                    "explanation": "Analysis completed but output format was unexpected.",
                    "execution_success": True,
                    "config": {
                        "model": self.config.gemini_model,
                        "timeout": self.config.execution_timeout,
                        "optimization": "maximum"
                    }
                }
                
        except Exception as e:
            logger.error(f"Error processing question: {str(e)}")
            return {
                "code": f"# Error occurred: {str(e)}",
                "output": {
                    "summary": f"Analysis failed: {str(e)}",
                    "data": {},
                    "visualizations": [],
                    "insights": [],
                    "metadata": {},
                    "status": "error",
                    "error": str(e)
                },
                "explanation": f"Error during analysis: {str(e)}",
                "execution_success": False,
                "config": {
                    "model": self.config.gemini_model,
                    "timeout": self.config.execution_timeout,
                    "optimization": "maximum"
                }
            }


# Self-test functionality
if __name__ == "__main__":
    print("=== ULTIMATE Agent Core Test Suite ===\n")
    
    # Initialize with optimal config
    config = AgentConfig(
        max_tokens=4000,
        temperature=0.2,
        enable_code_validation=True,
        enable_smart_parsing=True
    )
    
    # Test 1: Wikipedia question
    print("Test 1: Wikipedia Analysis")
    print("-" * 40)
    try:
        with open("test_questions/wikipedia_question.txt", "r") as f:
            wiki_task = f.read()
        
        print("Generating optimized script...")
        wiki_script = generate_analysis_script(wiki_task, config)
        print(f"âœ“ Generated {len(wiki_script)} characters")
        
        print("Executing script...")
        wiki_output = execute_script(wiki_script, config)
        wiki_json = json.loads(wiki_output)
        print(f"âœ“ Output: {wiki_json.get('summary', 'No summary')[:100]}")
        print(f"âœ“ Status: {wiki_json.get('status', 'unknown')}")
        
    except Exception as e:
        print(f"âœ— Wikipedia test failed: {e}")
    
    print("\n" + "="*50 + "\n")
    
    # Test 2: DuckDB question
    print("Test 2: DuckDB Analysis")
    print("-" * 40)
    try:
        with open("test_questions/duckdb_question.txt", "r") as f:
            duck_task = f.read()
        
        print("Generating optimized script...")
        duck_script = generate_analysis_script(duck_task, config)
        print(f"âœ“ Generated {len(duck_script)} characters")
        
        print("Executing script...")
        duck_output = execute_script(duck_script, config)
        duck_json = json.loads(duck_output)
        print(f"âœ“ Output: {duck_json.get('summary', 'No summary')[:100]}")
        print(f"âœ“ Status: {duck_json.get('status', 'unknown')}")
        
    except Exception as e:
        print(f"âœ— DuckDB test failed: {e}")
    
    print("\n" + "="*50)
    print("ðŸŽ¯ ULTIMATE Agent Core Ready!")
    print("âœ“ Maximum optimization applied")
    print("âœ“ Comprehensive prompt engineering")
    print("âœ“ Enhanced error handling")
    print("âœ“ Smart output parsing")
    print("\nReminder: Set GOOGLE_API_KEY environment variable for production use")